{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_length = 16\n",
    "n_embed = 4\n",
    "head_size = 8\n",
    "\n",
    "input = torch.randn((context_length, n_embed))\n",
    "\n",
    "query = nn.Linear(n_embed, head_size)\n",
    "key = nn.Linear(n_embed, head_size)\n",
    "value = nn.Linear(n_embed, head_size)\n",
    "\n",
    "q = query(input)\n",
    "k = key(input)\n",
    "v = value(input)\n",
    "# print(q.shape) # (context_length, head_size)\n",
    "\n",
    "# layman programmer's explanation of what mat mul achieves here\n",
    "# basically taking the dot product of each key <> query pair\n",
    "#\n",
    "#                4     4     5     8\n",
    "#                |     |     |     |\n",
    "#                2     5     3     4\n",
    "#                |     |     |     |\n",
    "#                2     2     4     4\n",
    "#                |     |     |     |\n",
    "#  2---3---0----{X}---{X}---{X}-->{X}\n",
    "#                |     |     |     |\n",
    "#  4---5---8----{X}---{X}---{X}-->{X}\n",
    "#                |     |     |     |\n",
    "#  5---3---4----{X}---{X}---{X}-->{X}\n",
    "#                v     v     v     v\n",
    "#  2---4---4----{X}---{X}---{X}-->{X}\n",
    "#\n",
    "attention_matrix = []\n",
    "for timestep in range(context_length):\n",
    "  attention_matrix.append([])\n",
    "  for other_timestep in range(context_length):\n",
    "    this_query = q[timestep]\n",
    "    this_key = k[other_timestep]\n",
    "    dot_product = float(this_query @ this_key)\n",
    "    attention_matrix[timestep].append(dot_product)\n",
    "\n",
    "attention_matrix2 = q @ k.T\n",
    "\n",
    "for i in range(context_length):\n",
    "  for j in range(context_length):\n",
    "    assert (attention_matrix[i][j] - attention_matrix2[i][j].item()) \\\n",
    "      < 1e-06\n",
    "\n",
    "# once we have the attention scores, we can use them to compute\n",
    "# a weighted sum of v across time for each timestep\n",
    "\n",
    "for timestep in range(context_length):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "picogpt-zG_Kpucb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
